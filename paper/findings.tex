\section{Findings and Future Work}
\label{sec:relwork}

%Related Work (``somewhat related'' work goes here; directly related work
%goes into the Introduction)~\cite{dsd13}.
\subsection{Analysis of Results}
\begin{table}[h!]
    \centering
    \begin{tabular}{| m{4cm} | m{0.75cm}| m{0.75cm} | m{0.75cm} |} \hline
        Vulnerability & $\#$ of Apps & Avg. $\#$ & Avg. $\#$ min. 1\\ \hline
        Unused Permissions & 90 & 6.01 & 6.48\\ \hline
        Dangerous Permission Combinations & 62 & 1.21 & 1.89\\ \hline            
        Unrequested Permissions & 0 & 0 & 0\\ \hline
        Overridden Trust Manager & 66 & 3.06 & 4.50\\ \hline
        Overridden Error Handler & 51  & 1.34 & 2.55\\ \hline
        Allow All Hostname Verifier & 20 & 0.25 & 1.26\\ \hline
        Mixed-Use SSL& 90 & 49.51 & 52.77 \\ \hline
        Improper Handling of addJavascriptInterface & 80 & 3.67 & 4.75 \\ \hline
    \end{tabular}
    \caption{Shows the number of apps that each vulnerability was found in, the average number of instances found of each vulnerability overall, and the average number of instances in apps where at least one instance was found.}
    \label{tab:my_label}
\end{table}

Through our experiments, we see that the majority of applications
(out of the 97 we analyzed) contain vulnerabilities within their
code. This indicates improper security practices are at play for 
most of the apps we analyzed.

When considering our sample size as representative of all 
applications on the Google Play store, the rate at which 
vulnerabilites are found is quite alarming. Many applications
available on the market, according to our study, have a significant 
chance of containing a vulnerability that allows for a third-party 
malicious attack.

For evaluating our results, we chose to invesitgate how each experiment potentially would produce false positives.

\textbf{Experiment 1}
False positives within the permissions experiment are not exactly an issue. Unused permissions would have a 100\% true positive rate (granted the Androguard library works as intended). 
This is due to the fact that all unsued permissions are a potentially vulnerable exploit that can always be avoided with developers checking and seeing what permissions are actually needed within the app. 

Dangerous permission combinations are not exactly evaluated the same as the other experiments (and other parts of this experiment). This is a test to simply see if any potentially dangerous combinations exist between 
the ones used within the app. 

\textbf{Experiment 2}
%False positives can arise for this experiment in situations where a safely implemented override occurs (i.e. a developer creates a new, safe trust manager or error handler). 
%It is also possible when a developer fully trusts a remote server, and feels they do not have to undergo the verification of a certificate.
False positives would occur for similar reasons in this experiment. 
Since the experiment looks for reimplementation of trust managers and error handlers,
if the developer rewrites one of these important methods, 
it's crucial for them to do so in a safe way. 
That involves actually checking for the security of a connection.

In order to check the validity of a positive, we selected a random
sample of 10 apps that supposedly contained overridden trust managers,
and 10 apps that supposedly contained overridden error handlers.
From these 10 apps each, we selected one instance of the vulnerability
to investigate further.
We then manually checked the smali code of each vulnerability's file location
 to see if the
overridden methods were implemented in a safe way.

For trust managers, 4 out of the 10 apps contained an unsafe implementation.
This indicates a false positive rate of 60\% for trust managers.
For error handlers, 1 out of the 10 apps contained an unsafe implementation.
This indicates a false positive rate of 90\% for error handlers.

These are high false positive rates, which indicate that the experiment
is not as accurate as we would desire.


\textbf{Experiment 3}
False positives for the AllowAllHostnameVerifier experiment may arise if an instance of the AllowAllHostnameVerifier 
class is found within an app, but is located in dead code. Manually verifying if an instance of a vulnerability is located 
in dead code, however, is not in the scope of this project. We assume that anytime this class is found to be implemented, the hostname verification process is not being carried out as intended, and thus the 
hostname within the server's certificate is not properly matched to that in the relevant server. 

\textbf{Experiment 4}
For the Mixed-Use SSL experiment, false positives may arise in situations where HTTP content being loaded by an HTTPS page is known and trusted by a developer. Additionally, there 
can be false positives if there exist strings that contain "http://" but never are used for a network call, or if HTTP is used within a local, secure environment. Within the scope of this
project, it is difficult to identify these false positives, as we do not have knowledge of the application's network structure and therefore what would be within the scope of a local, secure environment. 

\textbf{Experiment 5}
For the addJavascriptInterface() Method experiment, false positives can occur if the addJavascriptInterface() method is implemented without any @JavascriptInterface annotations in the class, but the developer 
trusts the source of the content being used within the webview. Once again, it is difficult to identify this type of false positive for this project, as we do not know which webviews are trusted by the developer. 

\subsection{Future Work}
If we were to continue this research in the future, we would 
want to explore some of the following areas. 

First and foremost, we would like to have more thorough experiments. 
Our experiments currently utilize the Androguard library in 
order to parse the apk files and find simple string matches. 
One example is from experiment two, where we search for 
overridden built-in methods. This experiment is fairly simplistic
and could be expanded upon into something where the method 
internals are also automatically checked to view if they are 
forgoing original intended use, making the user vulnerable.

Additionally, we would like to expand our set of Android 
applications to test on. If we were able to test on a larger 
set of apps, this would allow us to draw more accurate 
conclusions and notice more common trends in regards to 
vulnerabilities. This would allow us to develop a more 
comprehensive tool and experiments. 

Finally, instead of just utilizing static analysis, breaking 
into the domain of dynamic analysis could prove to be beneficial 
in seeing how these vulnerabilities can be exploited in real time. 
This would give us a chance to see how our predictions of 
vulnerabilities holds in a test environment of the application 
running. The results could prove valuable to enforce our 
findings and potentially find more vulnerabilities that were 
not found in static analysis. 
